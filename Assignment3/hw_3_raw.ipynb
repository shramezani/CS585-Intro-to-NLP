{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 3: Decoding Algorithms\n",
    "\n",
    "#### This is due at 11.55 pm on Friday, December 7. Please see detailed submission instructions below.  100 points total.\n",
    "\n",
    "##### How to do this problem set:\n",
    "\n",
    "- What version of Python should I use? 3.6\n",
    "\n",
    "- Most of these questions require writing Python code and computing results, and the rest of them have textual answers. To generate the answers, you will have to fill out two supporting files, `vit_starter.py` and `s2s_starter.py`.\n",
    "\n",
    "- Write all the answers in this ipython notebook. Once you are finished (1) Generate a PDF via (File -> Download As -> PDF) and upload to Gradescope and (2) turn in `vit_starter.py`, `s2s_starter.py`,  and `hw_3.ipynb` on Moodle. If you do the extra credit, repeat these two steps but upload them for the \"HW3 Extra Credit\" assignment.\n",
    "  \n",
    "- **Important:** Check your PDF before you turn it in to gradescope to make sure it exported correctly. If ipython notebook gets confused about your syntax it will sometimes terminate the PDF creation routine early. You are responsible for checking for these errors. If your whole PDF does not print, try running `$jupyter nbconvert --to pdf hw_1.ipynb` to identify and fix any syntax errors that might be causing problems.\n",
    "\n",
    "- **Important:** When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. Then you'll be sure it's actually right. One convenient way to do this is by clicking `Cell -> Run All` in the notebook menu.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Academic honesty \n",
    "\n",
    "- We will audit the Moodle code from a few dozen students, chosen at random. The audits will check that the code you wrote and turned on Moodle generates the answers you turn in on your Gradescope PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a potential case of cheating. See the course page for honesty policies.\n",
    "\n",
    "- We will also run automatic checks of code on Moodle for plagiarism. Copying code from others is considered a serious case of cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Viterbi (log-additive form) (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "One HMM chain is shown on the left. In the graph to the right, let $A(y_1,y_2)$ be the log of the transition probability when the model transition from $y_1$ to $y_2$. Similarly, let \n",
    "$B_t$ be the log of the emission probability when the model emits $w_t$ at $y_t$.\n",
    "\n",
    "\n",
    "Let $\\vec{y} = (y_1,y_2,...,y_T)$ be a proposed tag sequence for a $T$ length sentence.\n",
    "The total ***goodness function*** for a solution $\\vec{y}$ is\n",
    "\n",
    "$$ G(\\vec{y}) = \\sum_{t=1}^{T} B_t(y_t)  + \\sum_{t=2}^{T} A(y_{t-1},y_t) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1 (40 points)**\n",
    "\n",
    "Implement additive log-space Viterbi by completing the **viterbi()** function. It takes in tables that represent the $A$ and $B$ functions as input.  We give you an implementation of $G()$ in **vit_starter**, you can check to make sure you understand the data structures, and also the exhaustive decoding algorithm too.  Feel free to add debugging print statements as needed.  The main code runs the exercise example by default.\n",
    "\n",
    "When debugging, you should make new A and B examples that are very simple. This will test different code paths.  Also you can try the **randomized\\_test()** from the starter code.\n",
    "\n",
    "Look out for negative indexes as a bug.  In python, if you use an index that's too high to be in the list, it throws an error.  But it will silently accept a negative index ... it interprets that as indexing from the right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exhaustive decoding: [1, 1, 0] score: 32\n",
      "Viterbi    decoding: [1, 1, 0] score: 32\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Implement the viterbi() function in vit_starter.py and then run this cell to show your output\n",
    "\n",
    "from vit_starter import *\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    A = {(0,0):2, (0,1):1, (1,0):0, (1,1):5}\n",
    "    Bs= [ [0,1], [0,1], [25,0] ]\n",
    "    # that's equivalent to: [ {0:0,1:1}, {0:0,1:1}, {0:25,1:0} ]\n",
    "\n",
    "    y = exhaustive(A, Bs, set([0,1]))\n",
    "    print(\"Exhaustive decoding:\", y, \"score:\", goodness_score(y, A, Bs))\n",
    "    y = viterbi(A, Bs, set([0,1]))\n",
    "    print(\"Viterbi    decoding:\", y, \"score:\", goodness_score(y, A, Bs))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copy and paste the viterbi function that you implemented in `vit_starter.py`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(A, B, output_vocab):\n",
    "    \"\"\"\n",
    "    A: a dict of key:value pairs of the form\n",
    "        {(curtag,nexttag): score}\n",
    "    with keys for all K^2 possible neighboring combinations,\n",
    "    and scores are numbers.  We assume they should be used ADDITIVELY, i.e. in log space.\n",
    "    higher scores mean MORE PREFERRED by the model.\n",
    "\n",
    "    B: a list where each entry is a dict {tag:score}, so like\n",
    "    [ {Noun:-1.2, Adj:-3.4}, {Noun:-0.2, Adj:-7.1}, .... ]\n",
    "    each entry in the list corresponds to each position in the input.\n",
    "\n",
    "    output_vocab: a set of strings, which is the vocabulary of possible output\n",
    "    symbols.\n",
    "\n",
    "    RETURNS:\n",
    "    the tag sequence yvec with the highest goodness score\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(B)   # length of input sentence\n",
    "\n",
    "    # viterbi log-prob tables\n",
    "    V = [{tag:None for tag in output_vocab} for t in range(N)]\n",
    "    #print(\"v:\",V)\n",
    "    # backpointer tables\n",
    "    # back[0] could be left empty. it will never be used.\n",
    "    back = [{tag:None for tag in output_vocab} for t in range(N)]\n",
    "\n",
    "    # todo implement the main viterbi loop here\n",
    "    # you may want to handle the t=0 case separately\n",
    "    if N==0:\n",
    "        return[]\n",
    "    # Memoization\n",
    "    ## baseline  \n",
    "    path =[]\n",
    "    for i in V[0]:\n",
    "        V[0][i]=B[0][i]\n",
    "    \n",
    "    for t in range(1,len(V)):\n",
    "        for i in V[t]:\n",
    "            max_=0\n",
    "            max_index=-1\n",
    "            for j in V[t-1]:\n",
    "                if(V[t-1][j]+A[j,i]>max_):\n",
    "                    max_=V[t-1][j]+A[j,i]\n",
    "                    max_index=j\n",
    "            V[t][i] = V[t-1][max_index]+A[max_index,i]+B[t][i]\n",
    "            back[t][i] = max_index\n",
    "  \n",
    "\n",
    "\n",
    "    path.append(dict_argmax(V[N-1]))\n",
    "    for i in range( len(back)-1):\n",
    "        path.append(back[len(back)-i-1][path[i]])\n",
    "        \n",
    "    \n",
    "    #print(\"V: \",dict_argmax(V[N-1]))\n",
    "    #print(\"back: \",path)\n",
    "    # todo implement backtrace also\n",
    "    return list(reversed(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decoding in seq2seq models (60 points)\n",
    "\n",
    "In this part of the homework, you will implement both a greedy search and a beam search for a simple sequence-to-sequence model. We provide the code to build and train the network, so all you have to do is write the decoding algorithms. Please make sure PyTorch and numpy are installed properly before working on this section. \n",
    "\n",
    "Our sequence-to-sequence model consists of a vanilla RNN encoder and decoder. Given a sequence of characters (e.g., **aabbccdd**), the network is trained to produce the same sequence in reverse order (**ddccbbaa**). While this task is obviously not like machine translation or text summarization, the model and algorithms are the same, so we will use it as a proxy for more complex real-world tasks that require GPUs and huge datasets to train. \n",
    "\n",
    "To begin, run the below massive cell to (1) set up the data and vocab, (2) build the network, and (3) train the network. We will train for 50 epochs, which should hopefully take no more than a few minutes on your machine. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 50.869789\n",
      "epoch 10, loss 23.193577\n",
      "epoch 20, loss 15.435831\n",
      "epoch 30, loss 11.809231\n",
      "epoch 40, loss 11.643355\n",
      "epoch 49, loss 6.552234\n"
     ]
    }
   ],
   "source": [
    "import torch, random\n",
    "import numpy as np\n",
    "from s2s_starter import S2S\n",
    "\n",
    "# set random seeds\n",
    "torch.manual_seed(1111)\n",
    "random.seed(1111)\n",
    "np.random.seed(1111)\n",
    "\n",
    "# create a dataset of 500 examples using a small vocabulary \n",
    "# we will try training on sequences of length 10 and testing on sequences of length 15\n",
    "# this setup tests whether the model has actually learned an algorithm to reverse its input \n",
    "vocab = {'a': 0, 'b': 1, 'c':2, 'd':3, 'e':4}\n",
    "train_seq_len = 10\n",
    "test_seq_len = 15\n",
    "num_train_examples = 500\n",
    "num_test_examples = 100\n",
    "\n",
    "train_inputs = torch.LongTensor(num_train_examples, train_seq_len).random_(0, len(vocab)) # random sequences\n",
    "inv_idx = torch.arange(train_seq_len-1, -1, -1).long()\n",
    "train_outputs = train_inputs[:, inv_idx] # outputs are just the reverse of the input\n",
    "\n",
    "test_inputs = torch.LongTensor(num_test_examples, test_seq_len).random_(0, len(vocab))\n",
    "inv_idx = torch.arange(test_seq_len-1, -1, -1).long()\n",
    "test_outputs = test_inputs[:, inv_idx]\n",
    "\n",
    "    \n",
    "# build the network\n",
    "net = S2S(20, 50, len(vocab))\n",
    "\n",
    "# set some parameters for training the network\n",
    "batch_size = 16\n",
    "idx_to_w = dict((v,k) for (k,v) in vocab.items())\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "num_epochs = 50\n",
    "\n",
    "# okay, let's train the network!\n",
    "for ep in range(num_epochs):\n",
    "    ep_loss = 0.\n",
    "    \n",
    "    for start in range(0, len(train_inputs), batch_size):\n",
    "        in_batch = train_inputs[start:start+batch_size]\n",
    "        out_batch = train_outputs[start:start+batch_size]\n",
    "\n",
    "        preds = net(in_batch, out_batch)        \n",
    "        batch_loss = loss_fn(preds, out_batch.view(-1))\n",
    "        ep_loss += batch_loss\n",
    "\n",
    "        # compute gradients\n",
    "        optimizer.zero_grad() # reset the gradients from the last batch\n",
    "        batch_loss.backward() # does backprop!!!\n",
    "        optimizer.step() # updates parameters using gradients\n",
    "\n",
    "    if ep % 10 == 0 or ep == num_epochs - 1:\n",
    "        print('epoch %d, loss %f' % (ep, ep_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the loss go down to about 10. Now, let's try decoding some training sequences. In s2s.py, we have provided a greedy decoding algorithm (greedy_search) which just chooses the argmax character prediction at every time step. Run the below code to execute greedy search on the ***training*** data. You'll see the model's predictions for the first three training sentences, along with the accuracy on the entire training dataset. Accuracy here is defined as the percentage of examples for which we are able to exactly generate the reverse of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: a e a a a b d d e e\n",
      "out: e e d d e a c b c b, neg log prob: -1.910884\n",
      "\n",
      "inp: c d b e e c b b e e\n",
      "out: e e b b c e e b d c, neg log prob: -1.222504\n",
      "\n",
      "inp: d c a e d e c b b c\n",
      "out: c b b c e d e a c d, neg log prob: -1.170324\n",
      "\n",
      "training accuracy: 211 / 500, acc=42.2\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "probs = []\n",
    "corr = 0.\n",
    "total = 0.\n",
    "\n",
    "for seq_idx, seq in enumerate(train_inputs):\n",
    "\n",
    "    prob, outseq = net.greedy_search(seq.expand(1, train_seq_len))\n",
    "    inseq = ' '.join([idx_to_w[c] for c in seq.numpy()])\n",
    "    outseq = ' '.join([idx_to_w[int(c)] for c in outseq])\n",
    "    if seq_idx < 3:\n",
    "        print('inp: %s' % inseq)\n",
    "        print('out: %s, neg log prob: %f\\n' % (outseq, prob))\n",
    "    if inseq == outseq[::-1]:\n",
    "        corr += 1\n",
    "    total += 1\n",
    "    \n",
    "print('training accuracy: %d / %d, acc=%0.1f' % (corr, total, 100 * corr / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Implement beam search (40 points)\n",
    "These should look pretty decent! Most of the outputs may even be exact reverses of the input. With that said, we can do better. Implement the beam_search in s2s_starter.py and run the following cell. To debug, set the beam_size argument in the cell to 1 and make sure the output sequences and probabilities are identical to the ones produced by greedy search. If you have correctly implemented the function, the final line of output will print a 'success' message. You should also expect to see a higher accuracy than the greedy search! This cell may take a minute or so to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: a e a a a b d d e e\n",
      "beam0: e e d d b a a a e a, neg log prob: -1.341579\n",
      "input: a e a a a b d d e e\n",
      "beam1: e e d d e a c b c b, neg log prob: -1.910884\n",
      "input: a e a a a b d d e e\n",
      "beam2: e e d d e a b b a d, neg log prob: -2.779232\n",
      "\n",
      "input: c d b e e c b b e e\n",
      "beam0: e e b b c e e b d c, neg log prob: -1.222504\n",
      "input: c d b e e c b b e e\n",
      "beam1: e e b b c e e b c c, neg log prob: -2.687269\n",
      "input: c d b e e c b b e e\n",
      "beam2: e e b b c e e b d e, neg log prob: -2.881226\n",
      "\n",
      "input: d c a e d e c b b c\n",
      "beam0: c b b c e d e a c d, neg log prob: -1.170324\n",
      "input: d c a e d e c b b c\n",
      "beam1: c b b c e d e a c a, neg log prob: -1.781353\n",
      "input: d c a e d e c b b c\n",
      "beam2: c b b c e d e a d d, neg log prob: -2.298801\n",
      "\n",
      "training accuracy: 236 / 500, acc=47.2\n",
      "success! you've successfully implemented beam search!\n"
     ]
    }
   ],
   "source": [
    "passed_check = True\n",
    "corr = 0.\n",
    "total = 0.\n",
    "\n",
    "for seq_idx, seq in enumerate(train_inputs):\n",
    "    beams = net.beam_search(seq.expand(1, train_seq_len), beam_size=3)\n",
    "    inseq = ' '.join([idx_to_w[c] for c in seq.numpy()])\n",
    "    for beam_idx, beam in enumerate(beams):\n",
    "        prob = beam[0]\n",
    "        outseq = beam[1]\n",
    "        \n",
    "        if isinstance(prob, torch.Tensor):\n",
    "            prob = prob.detach().numpy()\n",
    "        outseq = ' '.join([idx_to_w[int(c)] for c in outseq])\n",
    "        if seq_idx < 3:\n",
    "            print('input: %s' % inseq)\n",
    "            print('beam%d: %s, neg log prob: %f' % (beam_idx, outseq, prob))\n",
    "            \n",
    "        if beam_idx == 0:\n",
    "            if inseq == outseq[::-1]:\n",
    "                corr += 1\n",
    "            total += 1\n",
    "\n",
    "    if seq_idx < 3:\n",
    "        print('')\n",
    "    if(net.beam_check(seq.expand(1, train_seq_len)) == False):\n",
    "        passed_check = False\n",
    "        \n",
    "print('training accuracy: %d / %d, acc=%0.1f' % (corr, total, 100 * corr / total))\n",
    "if passed_check:\n",
    "    print(\"success! you've successfully implemented beam search!\")\n",
    "else:\n",
    "    print(\"your beam search has bugs, go back and check your code carefully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 (10 pts)\n",
    "What is the maximum beam size we can use? Why? write answer here\n",
    "\n",
    "\n",
    "Total number of possible sequences with length equal to maximum length, which means ; length of vocabulary to the power of maximum sequence length. Since we don't have more than this sequence so if we use beam size bigger than this number the we will waste the space (in this case we are choosing most probable result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 (10 pts)\n",
    "Is beam search always guaranteed to find an output whose probability is greater than or equal to the output of greedy search? Why? ***No, Since beam search algorithm alwasy picks k most most proboble sequence among current sequences. Lets say we have beam size of 2, in first step we have two probabilities $(a , b ; a>b)$--> greedy algorithm picks $a$ and beam search algorithm picks $a$ and $b$. In second step beam search extends $a$ to $a1,a2$ and $b$ to $b1,b2$  and picks two of them. if probability of $(b1>b2>a1>a2)$ beam search picks $b1$ and $b2$ while greedy picks $a1$. At this time, beam search algorithm misses $a1$ path so if we expand $a1$ to $a3,a4$ and $b1$ to $b3,b4$ and $b2$ to $b5,b6$ and if probability of $a3>b3>b4>b5>b6$ then greedy algorithm picks most probable sequence(which is $[start,a1,a3]$) among all sequences up to know while beam search algorithm selects a sequence with less probability $[start,b1,b3]$***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra credit (up to 30 pts)\n",
    "Before starting the extra credit, please export your current notebook to a PDF and upload it to Gradescope, as you may want to modify previous parts of this homework to solve the extra credit. Once you finish the extra credit, export this notebook again to a new PDF and upload it under the separate \"HW3 extra credit\" assignment on Gradescope. \n",
    "\n",
    "You have a simple goal: achieve over 30% accuracy on the ***test*** set. If you do not reach this number, you will get no points on this extra credit. The below cell runs beam search over the test data and computes the accuracy, which will likely be 0%. Feel free to do anything you like (generate more training data, train for more epochs, use bigger beams, use more powerful models, implement an attention mechanism, etc.) as long as you ***(1) do not hard-code the reverse algorithm anywhere (e.g., return inputs[::-1]) and (2) do not train on input sequences of longer than length 10***. If you succeed, your model will have generalized the reverse algorithm from length <10 to length 15! One thing that might be helpful to do first is to try overfitting your training data (i.e., make its accuracy 100%). Finally ***write what you did to achieve at least 30% accuracy here, or you will receive no points***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = 0.\n",
    "total = 0.\n",
    "\n",
    "for seq in test_inputs:\n",
    "    beams = net.beam_search(seq.expand(1, test_seq_len), beam_size=3)\n",
    "    \n",
    "    inseq = ' '.join([idx_to_w[c] for c in seq.numpy()])\n",
    "    prob = beams[0][0]\n",
    "    outseq = beams[0][1]\n",
    "    prob = prob.detach().numpy()\n",
    "    outseq = ' '.join([idx_to_w[int(c)] for c in outseq])\n",
    "    if inseq == outseq[::-1]:\n",
    "        corr += 1\n",
    "    total += 1\n",
    "\n",
    "print('%d / %d, test accuracy is %0.1f' % (corr, total, 100 * corr / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
