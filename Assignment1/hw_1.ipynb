{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem set 1, Intro to NLP 2018\n",
    "\n",
    "#### This is due on September 25, 2018, submitted electronically. 100 points total.\n",
    "\n",
    "##### How to do this problem set:\n",
    "\n",
    "- What version of Python should I use? 3.6!\n",
    "\n",
    "- Most of these questions require writing Python code and computing results, and the rest of them have textual answers. To generate the answers, you will have to fill out a supporting file, `hw1.py`.\n",
    "\n",
    "- For all of the textual answers you have to fill out have placeholder text which says \"Answer in one or two sentences here.\" For each question, you need to replace \"Answer in one or two sentences here\" with your answer.\n",
    "\n",
    "- Write all the answers in this ipython notebook. Once you are finished (1) Generate a PDF via (File -> Download As -> PDF) and upload to Gradescope (2)Turn in `hw_1.py` and `hw_1.ipynb` on Moodle.\n",
    "  \n",
    "- **Important** check your PDF before you turn it in to gradescope to make sure it exported correctly. If ipython notebook gets confused about your syntax it will sometimes terminate the PDF creation routine early. If your whole PDF does not print, try running `$jupyter nbconvert --to pdf 2018hw1.ipynb` to identify and fix any syntax errors that might be causing problems\n",
    "\n",
    "- When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. Then you'll be sure it's actually right. One handy way to do this is by clicking `Cell -> Run All` in the notebook menu.\n",
    " \n",
    "- This assignment is designed so that you can run all cells in a few minutes of computation time. If it is taking longer than that, you probably have made a mistake in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Academic honesty \n",
    "\n",
    "- We will audit the Moodle code from a set number of students, chosen at random. The audits will check that the code you wrote and turned on Moodle generates the answers you turn in on your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. See the course page for honesty policies.\n",
    "\n",
    "- We will also run automatic checks of code on Moodle for plagiarism. Copying code from others is also considered a serious case of cheating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell! It sets some things up for you.\n",
    "\n",
    "# This code makes plots appear inline in this document rather than in a new window.\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import division  # this line is important to avoid unexpected behavior from division\n",
    "\n",
    "# This code imports your work from hw_1.py\n",
    "from hw_1 import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5, 4) # set default size of plots\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! You have 12500 pos reviews in Dataset\\large_movie_review_dataset\\train/pos\n",
      "Great! You have 12500 neg reviews in Dataset\\large_movie_review_dataset\\train/neg\n"
     ]
    }
   ],
   "source": [
    "# download the IMDB large movie review corpus from https://people.cs.umass.edu/~miyyer/cs585/homeworks/data/large_movie_review_dataset.zip to a file location on your computer\n",
    "\n",
    "PATH_TO_DATA = 'Dataset\\large_movie_review_dataset'  # set this variable to point to the location of the IMDB corpus on your computer\n",
    "POS_LABEL = 'pos'\n",
    "NEG_LABEL = 'neg'\n",
    "TRAIN_DIR = os.path.join(PATH_TO_DATA, \"train\")\n",
    "TEST_DIR = os.path.join(PATH_TO_DATA, \"test\")\n",
    "\n",
    "for label in [POS_LABEL, NEG_LABEL]:\n",
    "    if len(os.listdir(TRAIN_DIR + \"/\" + label)) == 12500:\n",
    "        print (\"Great! You have 12500 {} reviews in {}\".format(label, TRAIN_DIR + \"/\" + label))\n",
    "    else:\n",
    "        print (\"Oh no! Something is wrong. Check your code which loads the reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I received this movie as a gift, I knew from the DVD cover, this movie are going to be bad.After not watching it for more than a year I finally watched it. what a pathetic movie",
      ".<br /><br />I almost didn't finish watching this bad movie,but it will be unfair of me to write a review without watching the complete movie.<br /><br />Trust me when I say \" this movie sucks\" I am truly shocked that some bad filmmaker wane bee got even financed to make this pathetic movie, But it couldn't have cost more than $20 000 to produce this movie. all you need are a cheap camcorder or a cell phone camera .about 15 people with no acting skills, a scrip that were written by a couple of drunk people.<br /><br />In the fist part of this ultra bad move a reporter (Tara Woodley )run a suppose to be drunk man over on her way to report on a hunted town. He are completely unharmed. They went to a supposed to be abandon house ,but luckily for the it almost complete furnished and a bottle of liquor on the door step happens to be there. just for the supposed to be drunk man but all is not what it seems.<br /><br />Then the supposed drunk man start telling Tara ghost/zombies stories.<br /><br />The fist of his stupid lame stories must be the worst in history.<br /><br />his story<br /><br />Sgt. Ben Draper let one of his soldiers die of complete exhaustion (I think this is what happens)after letting the poor soldier private Wilson do sit ups he let him dig a grave and then the soldier collapse ,Ben Draper<br /><br />buries him in a shallow grave.<br /><br />But Sgt. Ben Draper are in for n big surprise. his wife/girl fiend knows about this and she and her lover kills Sgt. Ben Draper to take revenge on private Wilson.(next to the grave of the soldier he sort off murdered) The soldier wakes up from his grave in the form of zombie and kill them for taking revenge on his behalf.<br /><br />The twist ending were so lame.<br /><br />Even if you like B HORROR movies, don't watch this movie\n"
     ]
    }
   ],
   "source": [
    "# Actually reading the data you are working with is an important part of NLP! Let's look at one of these reviews\n",
    "\n",
    "print (open(TRAIN_DIR + \"/neg/98_1.txt\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part One: Intro to NLP in Python: types, tokens and Zipf's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types and tokens\n",
    "\n",
    "One major part of any NLP project is word tokenization. Word tokenization is the task of segmenting text into individual words, called tokens. In this assignment, we will use simple whitespace tokenization. Take a look at the `tokenize_doc` function in `hw_1.py`. **You should not modify tokenize_doc** but make sure you understand what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already\n",
      "semester!\n",
      "this\n",
      "is\n",
      "favorite\n",
      "cmpsci\n",
      "585\n",
      "my\n",
      "class\n"
     ]
    }
   ],
   "source": [
    "# We have provided a tokenize_doc function in hw_1.py. Here is a short demo of how it works\n",
    "\n",
    "d1 = \"This SAMPLE doc has   words tHat  repeat repeat\"\n",
    "bow = tokenize_doc(d1)\n",
    "\n",
    "assert bow['this'] == 1\n",
    "assert bow['sample'] == 1\n",
    "assert bow['doc'] == 1\n",
    "assert bow['has'] == 1\n",
    "assert bow['words'] == 1\n",
    "assert bow['that'] == 1\n",
    "assert bow['repeat'] == 2\n",
    "\n",
    "bow2 = tokenize_doc(\"CMPSCI 585 is already my favorite class this semester!\")\n",
    "for b in bow2:\n",
    "    print (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to look at the word types and word tokens in the corpus.\n",
    "Use the `word_counts` dictionary variable to store the count of each word in the corpus.\n",
    "Use the `tokenize_doc` function to break documents into tokens. **You should not modify tokenize_doc** but make sure you understand what it is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.1 (5 points)**\n",
    "\n",
    "Complete the cell below to fill out the `word_counts` dictionary variable. `word_counts` keeps track of how many times a word type appears across the corpus. For instance, `word_counts[\"movie\"]` should store the number 61492 -- the count of how many times the word `movie` appears in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import codecs\n",
    "word_counts = Counter() # Counters are often useful for NLP in python\n",
    "\n",
    "for label in [POS_LABEL, NEG_LABEL]:\n",
    "    for directory in [TRAIN_DIR, TEST_DIR]:\n",
    "        for fn in glob.glob(directory + \"/\" + label + \"/*txt\"):\n",
    "            ## TODO: complete me! \n",
    "            doc = codecs.open(fn, 'r', 'utf8') # Open the file with UTF-8 encoding\n",
    "            bow_each_review = tokenize_doc(doc.read())\n",
    "            #txt = open(fn).read().decode('utf8')\n",
    "            #bow_each_review = tokenize_doc(txt)\n",
    "            for word in bow_each_review:\n",
    "                word_counts[word] += int(bow_each_review[word])\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yay! there are 61492 total instances of the word type movie in the corpus\n"
     ]
    }
   ],
   "source": [
    "# you should see 61492 instances of the word type \"movie\" in the corpus. \n",
    "if word_counts[\"movie\"] == 61492:\n",
    "    print (\"yay! there are {} total instances of the word type movie in the corpus\".format(word_counts[\"movie\"]))\n",
    "else:\n",
    "    print (\"hmm. Something seems off. Double check your code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2 (5 points)**\n",
    "\n",
    "Take a look at the following values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 390931 word types in the corpus\n",
      "there are 11557847 word tokens in the corpus\n"
     ]
    }
   ],
   "source": [
    "print (\"there are {} word types in the corpus\".format(n_word_types(word_counts)))\n",
    "print (\"there are {} word tokens in the corpus\".format(n_word_tokens(word_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a much higher number of tokens than types. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Because number of tokens represents number of all words in the corpus containg repeatition of each word but number of types represents number of unique words in the corpus withous considering repeatition of each word (it is our vocabulary size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.3 (5 points)**\n",
    "\n",
    "Using the word_counts dictionary you just created, make a new dictionary called sorted_dict where the words are sorted according to their counts, in decending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement me!\n",
    "sorted_list = sorted(word_counts.items(),key = operator.itemgetter(1),reverse = True) #sort with respect to value\n",
    "sorted_dict =  sorted_list \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the first 30 values from sorted_dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 638861\n",
      "a 316615\n",
      "and 313637\n",
      "of 286661\n",
      "to 264573\n",
      "is 204876\n",
      "in 179807\n",
      "i 141587\n",
      "this 138483\n",
      "that 130140\n",
      "it 129614\n",
      "/><br 100974\n",
      "was 93258\n",
      "as 88242\n",
      "with 84590\n",
      "for 84510\n",
      "but 77864\n",
      "on 62890\n",
      "movie 61492\n",
      "are 57009\n",
      "his 56870\n",
      "not 56765\n",
      "you 55600\n",
      "film 55086\n",
      "have 54423\n",
      "he 51062\n",
      "be 50901\n",
      "at 45259\n",
      "one 44983\n",
      "by 43359\n"
     ]
    }
   ],
   "source": [
    "# Implement me!\n",
    "n = 30\n",
    "#import collections\n",
    "#print(collections.Counter(sorted_dict).most_common(30))\n",
    "for k in range(n):\n",
    "    print sorted_dict[k][0],sorted_dict[k][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zipf's Law\n",
    "\n",
    "**Question 1.4 (5 points)**\n",
    "\n",
    "In this section, you will verify a key statistical properties of text: [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law).\n",
    "\n",
    "Zipf's Law describes the relations between the frequency rank of words and frequency value of words.  For a word $w$, its frequency is inversely proportional to its rank:\n",
    "\n",
    "$$count_w = K \\frac{1}{rank_w}$$\n",
    "or in other words\n",
    "$$\\log(count_w) = K - \\log(rank_w)$$\n",
    "\n",
    "for some constant $K$, specific to the corpus and how words are being defined.\n",
    "\n",
    "Therefore, if Zipf's Law holds, after sorting the words descending on frequency, word frequency decreases in an approximately linear fashion under a log-log scale.\n",
    "\n",
    "Please make such a log-log plot by ploting the rank versus frequency **Hint: Make use of the sorted dictionary you just created.**.  Use a scatter plot where the x-axis is the *log(rank)*, and y-axis is *log(frequency)*.  You should get this information from `word_counts`; for example, you can take the individual word counts and sort them.  dict methods `.items()` and/or `values()` may be useful.  (Note that it doesn't really matter whether ranks start at 1 or 0 in terms of how the plot comes out.) You can check your results by comparing your plots to ones on Wikipedia; they should look qualitatively similar.\n",
    "\n",
    "*Please remember to label the meaning of the x-axis and y-axis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x10476ba8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAEKCAYAAAB0cRxpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHJRmZjU4KM3Mavj/UljATIpIMc8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7951128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import operator\n",
    "x = []\n",
    "y = []\n",
    "X_LABEL = \"log(rank)\"\n",
    "Y_LABEL = \"log(frequency)\"\n",
    "\n",
    "for i in range(len(sorted_dict)):\n",
    "    y.append(math.log(sorted_dict[i][1]))\n",
    "    x.append(math.log(i+1))\n",
    "\n",
    "\n",
    "# implement me! you should fill the x and y arrays. Add your code here\n",
    "\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.grid()\n",
    "plt.xlabel(X_LABEL)\n",
    "plt.ylabel(Y_LABEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.5 (5 points)**\n",
    "\n",
    "You should see some discontinuities on the left and right sides of this figure.  Why are we seeing them on the left?  Why are we seeing them on the right?  On the right, what are those \"ledges\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "- The left part represents most frequent words and shows that frequncy of most frequent words are almost far from each other and these unique most frequent words are very little in the corpus so that in the head of Zipf's distribution they are far from each other and we can see the points related to them are separate from each other, but the rigth part or tail of Zipf's distribution represents the least frequent words and shows that number of least frequent words are a lot which means there are many unique words with frequency less than 10.\n",
    "\n",
    "\n",
    "\n",
    "- The ledges represent that there are many words with frequency equal to one and there are many words with frequency equal two but the words with frequency = 1 are much more than words with frequency = 2 and so on( number_of_words_with(freq=1)>> number_of_words_withf(freq=2)>number_of_words_with(freq=3)> ... ) and the number of words with frequency = k has a inverse correlation with k. Also it shows that lots of types in the corpus (around half of them) have frequency eqal to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the homework will walk you through coding a Naive Bayes classifier that can distinguish between positive and negative reviews (at some level of accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.1 (10 pts) ** \n",
    "\n",
    "To start, implement the `update_model` function in `hw1.py`. Make sure to read the function comments so you know what to update. Also review the NaiveBayes class variables in the `def __init__` method of the NaiveBayes class  to get a sense of which statistics are important to keep track of. Once you have implemented `update_model`, run the train model function using the code below. What is the size of the vocabulary used in the training documents? You’ll need to provide the path to the dataset you downloaded to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPORTING CORPUS STATISTICS\n",
      "('NUMBER OF DOCUMENTS IN POSITIVE CLASS:', 12500.0)\n",
      "('NUMBER OF DOCUMENTS IN NEGATIVE CLASS:', 12500.0)\n",
      "('NUMBER OF TOKENS IN POSITIVE CLASS:', 2958832.0)\n",
      "('NUMBER OF TOKENS IN NEGATIVE CLASS:', 2885848.0)\n",
      "('VOCABULARY SIZE: NUMBER OF UNIQUE WORDTYPES IN TRAINING CORPUS:', 251637)\n",
      "Great! The vocabulary size is 251637\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes(PATH_TO_DATA, tokenizer=tokenize_doc)\n",
    "nb.train_model()\n",
    "\n",
    "\n",
    "if len(nb.vocab) == 251637:\n",
    "    print (\"Great! The vocabulary size is {}\".format(251637))\n",
    "else:\n",
    "    print (\"Oh no! Something seems off. Double check your code before continuing. Maybe a mistake in update_model?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory analysis\n",
    "\n",
    "Let’s begin to explore the count statistics stored by the update model function. Implement the provided `top_n` function to find the top 10 most common words in the positive class and top 10 most common words in the negative class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 WORDS FOR CLASS pos:\n",
      "('', 'the', 165805.0)\n",
      "('', 'and', 87029.0)\n",
      "('', 'a', 82055.0)\n",
      "('', 'of', 76155.0)\n",
      "('', 'to', 65869.0)\n",
      "('', 'is', 55785.0)\n",
      "('', 'in', 48422.0)\n",
      "('', 'i', 33143.0)\n",
      "('', 'it', 32802.0)\n",
      "('', 'that', 32705.0)\n",
      "()\n",
      "TOP 10 WORDS FOR CLASS neg:\n",
      "('', 'the', 156393.0)\n",
      "('', 'a', 77898.0)\n",
      "('', 'and', 71543.0)\n",
      "('', 'of', 68307.0)\n",
      "('', 'to', 68098.0)\n",
      "('', 'is', 48386.0)\n",
      "('', 'in', 42105.0)\n",
      "('', 'i', 37337.0)\n",
      "('', 'this', 37301.0)\n",
      "('', 'that', 33587.0)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print (\"TOP 10 WORDS FOR CLASS \" + POS_LABEL + \":\")\n",
    "for tok, count in nb.top_n(POS_LABEL, 10):\n",
    "    print ('', tok, count)\n",
    "print ()\n",
    "\n",
    "print (\"TOP 10 WORDS FOR CLASS \" + NEG_LABEL + \":\")\n",
    "for tok, count in nb.top_n(NEG_LABEL, 10):\n",
    "    print ('', tok, count)\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.2 (5 points)**\n",
    "\n",
    "What is the first thing that you notice when you look at the top 10 words for the 2 classes? Are these words helpful for discriminating between the two classes? Do you think this trend carries forward to other texts from the English language? What about other languages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "They are almost the same so it is not helpful for discriminating between the two classes.\n",
    "Yes. Yes in each language there are these kind of words that are very frequent and they dont have any information( stop words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.3 (10 pts) **\n",
    "\n",
    "The Naive Bayes model assumes that all features are conditionally independent given the class label. For our purposes, this means that the probability of seeing a particular word in a document with class label $y$ is independent of the rest of the words in that document. Implement the `p_word_given_label` function. This function calculates P (w|y) (i.e., the probability of seeing word w in a document given the label of that document is y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your `p_word_given_label` function to compute the probability of seeing the word “amazing” given each sentiment label. Repeat the computation for the word “dull.” "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"P('amazing'|pos):\", 0.00026158970837141145)\n",
      "(\"P('amazing'|neg):\", 7.207586816769282e-05)\n",
      "(\"P('dull'|pos):\", 3.278320634628799e-05)\n",
      "(\"P('dull'|neg):\", 0.00014311218054450546)\n"
     ]
    }
   ],
   "source": [
    "print (\"P('amazing'|pos):\",  nb.p_word_given_label(\"amazing\", POS_LABEL))\n",
    "print (\"P('amazing'|neg):\",  nb.p_word_given_label(\"amazing\", NEG_LABEL))\n",
    "print (\"P('dull'|pos):\",  nb.p_word_given_label(\"dull\", POS_LABEL))\n",
    "print (\"P('dull'|neg):\",  nb.p_word_given_label(\"dull\", NEG_LABEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which word has a higher probability, given the positive class? Which word has a higher probability, given the negative class? Is this behavior expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "- Dull\n",
    "- Amazing\n",
    "- In general dull seems to be a word with negative sentiment and amazing seems to be a word with positive sentiment. So it is expected that the high probable word in negative documents shouldn't be the word amazing which is a positive word( and the same for dull and positive documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the purpose of the independence assumption for the Naive Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "- Make the process easier and faster by reducing number of parameters\n",
    "- It has an acceptable accuracy in comparision with other models in some problem and performs well despite this property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.4 (5 pts)**\n",
    "\n",
    "In the next cell, compute the probability of the word \"stop-sign.\" in the positive training data and negative training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"P('stop-sign.'|pos):\", 0.0)\n",
      "(\"P('stop-sign.'|neg):\", 3.4651859696006166e-07)\n"
     ]
    }
   ],
   "source": [
    "print (\"P('stop-sign.'|pos):\",  nb.p_word_given_label(\"stop-sign.\", POS_LABEL))\n",
    "print (\"P('stop-sign.'|neg):\",  nb.p_word_given_label(\"stop-sign.\", NEG_LABEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is unusual about P('stop-sign.'|pos)? Why is this a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer:**\n",
    "- It's probability is zero\n",
    "- Because in Naive Bayes we multiply probability of words in computing joint probability and if one of them is zero, it makes the result zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.5 (5 pts)**\n",
    "\n",
    "We can address the issues from question 2.4 with add-$\\alpha$ smoothing (like add-1 smoothing except instead of adding 1 we add $\\alpha$). Implement\n",
    "`p_word_given_label_and_alpha` and then run the next cell. Hint: look at the slides from the lecture and the corresponding exercise on add-1 smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"P('stop-sign.'|pos):\", 6.646374399441918e-08)\n"
     ]
    }
   ],
   "source": [
    "print (\"P('stop-sign.'|pos):\",  nb.p_word_given_label_and_alpha(\"stop-sign.\", POS_LABEL, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.6 (5 pts)** \n",
    "\n",
    "*Prior and Likelihood* \n",
    "\n",
    "As noted before, the Naive Bayes model assumes that all words in a document are independent of one another given the document’s label. Because of this we can write the likelihood of a document as:\n",
    "\n",
    "$P(w_{d1},\\cdots,w_{dn}|y_d) = \\prod_{i=1}^{n}P(w_{di}|y_d)$\n",
    "\n",
    "However, if a document has a lot of words, the likelihood will become extremely small and we’ll encounter numerical underflow. Underflow is a common problem when dealing with probabilistic models; if you are unfamiliar with it, you can get a brief overview on [Wikipedia](https:/en.wikipedia.org/wiki/Arithmetic_underflow). To deal with underflow, a common transformation is to work in log-space.\n",
    "\n",
    "$\\log[P(w_{d1},\\cdots,w_{dn}|y_d)] = \\sum_{i=1}^{n}\\log[P(w_{di}|y_d)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the log_likelihood function (Hint: it should make calls to the p word given label and alpha function). Implement the log_prior function. This function takes a class label and returns the log of the fraction of the training documents that are of that label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.7 (5 pts) **\n",
    "\n",
    "Naive Bayes is a model that tells us how to compute the posterior\n",
    "probability of a document being of some label (i.e.,\n",
    "$P(y_d|\\mathbf{w_d})$).  Specifically, we do so using bayes rule:\n",
    "\n",
    "  $P(y_d|\\mathbf{w_d}) = \\frac{P(y_d)P(\\mathbf{w_d}|y_d)}{P(\\mathbf{w_d})}$\n",
    "\n",
    "In the previous section you implemented functions to compute both\n",
    "the log prior ($\\log[P(y_d)]$) and the log likelihood\n",
    "($\\log[P( \\mathbf{w_d} |y_d)]$ ). Now, all you're missing is the\n",
    "*normalizer*, $P(\\mathbf{w_d})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the normalizer by expanding $P(\\mathbf{w_d})$.<br\\> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Without smoothing:\n",
    "\n",
    "$P(\\mathbf{w_d})= \\prod_{i=1}^{n} P(w_{di})\n",
    "                       = \\prod_{i=1}^{n}(\\frac{count(w_{di})}{\\sum_{j=1}^{n} count(w_{dj})})$\n",
    "                       \n",
    "$\\log P(\\mathbf{w_d})= \\sum_{i=1}^{n}\\log P(w_{di})\n",
    "                       = \\sum_{i=1}^{n}\\log(\\frac{count(w_{di})}{\\sum_{j=1}^{n} count(w_{dj})})$\n",
    "                       \n",
    "With smoothing:\n",
    "\n",
    "$P(\\mathbf{w_d})= \\prod_{i=1}^{n} P(w_{di})\n",
    "                       = \\prod_{i=1}^{n}(\\frac{count(w_{di})+1}{\\sum_{j=1}^{n} count(w_{dj})+n})$\n",
    "                       \n",
    "$\\log P(\\mathbf{w_d})= \\sum_{i=1}^{n}\\log P(w_{di})\n",
    "                       = \\sum_{i=1}^{n}\\log(\\frac{count(w_{di})+1}{\\sum_{j=1}^{n} count(w_{dj})+n})$\n",
    "\n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.8 (5 pts)**\n",
    "\n",
    "One way to classify a document is to compute the unnormalized log posterior for both labels and take the argmax (i.e., the label that yields the higher unnormalized log posterior). The unnormalized log posterior is the sum of the log prior and the log likelihood of the document. Why don’t we need to compute the log normalizer here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "- Because it is same for both labels so it can be droped out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.9 (5 pts)**\n",
    "\n",
    "As we saw earlier, the top 10 words from each class do not give us much to go on when classifying a document. A much more powerful metric is the likelihood ratio, which is defined as\n",
    "\n",
    "$LR(w)=\\frac{P(w|y=\\mathrm{pos})}{P(w|y=\\mathrm{neg})}$\n",
    "\n",
    "A word with LR 3 is 3 times more likely to appear in the positive class than in the negative. A word with LR 0.3 is one-third as likely to appear in the positive class as opposed to the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"LIKELIHOOD RATIO OF 'amazing':\", 3.628350587556548)\n",
      "(\"LIKELIHOOD RATIO OF 'dull':\", 0.22953174277018223)\n",
      "(\"LIKELIHOOD RATIO OF 'and':\", 1.1869527527674362)\n",
      "(\"LIKELIHOOD RATIO OF 'to':\", 0.9438077915764572)\n"
     ]
    }
   ],
   "source": [
    "# Implement the nb.likelihood_ratio function and use it to investigate the likelihood ratio of \"amazing\" and \"dull\"\n",
    "print (\"LIKELIHOOD RATIO OF 'amazing':\", nb.likelihood_ratio('amazing', 0.2))\n",
    "print (\"LIKELIHOOD RATIO OF 'dull':\", nb.likelihood_ratio('dull', 0.2))\n",
    "print (\"LIKELIHOOD RATIO OF 'and':\", nb.likelihood_ratio('and', 0.2))\n",
    "print (\"LIKELIHOOD RATIO OF 'to':\", nb.likelihood_ratio('to', 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the minimum and maximum possible values the likelihood ratio can take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer: **\n",
    "$(0,\\infty)$\n",
    "\n",
    "If there is not any of a word w in positive documents and the number of words in positive document goes to infinity this ratio will goes to zero --> the minimum possible value could be very very small but not zero (because of ratio = 0.2) and is equal to$\n",
    "    \\frac{0.2 * (count(WordsInNegativeDocs)+|v|*0.2)}{ (count(WordsInPositiveDocs)+|v|*0.2) * (count(TheGivenWordInNegativeDocs) + 0.2 )}$ that the number of given word in the negative docs could be equal to number of words in negative docs  if the |V| be very small and number of words in positive docs be very very large it goes to zero the reverse senario will lead the ratio equal to inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the word in the vocabulary with the highest likelihood ratio below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'edie', 401.0316267725798)\n"
     ]
    }
   ],
   "source": [
    "# Implement me!\n",
    "# Print the word with the highest likelihood ratio here\n",
    "highest_ratio = -1\n",
    "candidate_word = ''\n",
    "for word in nb.vocab:\n",
    "    curr_ratio = nb.likelihood_ratio(word,0.2)\n",
    "    if curr_ratio>highest_ratio:\n",
    "        highest_ratio = curr_ratio\n",
    "        candidate_word = word\n",
    "print (candidate_word,highest_ratio)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.10 (5 pts)**\n",
    "\n",
    "The unnormalized log posterior is the sum of the log prior and the log likelihood of the document. Implement the `unnormalized_log_posterior` function and the `classify` function. The `classify` function should use the unnormalized log posteriors but should not compute the normalizer. Once you implement the `classify` function, we'd like to evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.176\n"
     ]
    }
   ],
   "source": [
    "print (nb.evaluate_classifier_accuracy(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.11 (5 pts)**\n",
    "\n",
    "Try evaluating your model again with a smoothing parameter of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.176\n"
     ]
    }
   ],
   "source": [
    "print (nb.evaluate_classifier_accuracy(1000.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the accuracy go up or down when the pseudo count parameter is raised to 1000? Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The accuracy decreased.***\n",
    "Since in calculating the unnrmalized log posterior the log prior can affect the result so if the alpha is very high with respect to number of words in docs the log likelihood will converge to log(1/|V|) for both labels and increase the affect of log prior which leads wrong classification. But 1000 is not bigger than number of words in documents which means it is not the case.\n",
    "In this case the alpha is equal to 1000 and number of words in each class is approximately equal to 2900000 so in calculating log likelihood it doesn't really affect the denominator since the denominator is approximately equal to 2900000 + |V|* 1000. But nominator of log likelihood is very small with respect to alpha ( according to Zipf's law frequency of half of words is 1) so it has a high affect on nominator of log likelihood which means it can decrease the affect of number of words in a class (which has a direct relation to the log prior) and can decrease the affect of log prior effectively and lead misclassiffication.So this parameter can decrease or increase the affect of log priority which is a trade-off and need to be selected carefully (not verly small not very large) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 2.12 (5 pts)** \n",
    "\n",
    "Find a review that your classifier got wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong label: neg\n",
      "I loved this movie from beginning to end.I am a musician and i let drugs get in the way of my some of the things i used to love(skateboarding,drawing) but my friends were always there for me.Music was like my rehab,life support,and my drug.It changed my life.I can totally relate to this movie and i wish there was more i could say.This movie left me speechless to be honest.I just saw it on the Ifc channel.I usually hate having satellite but this was a perk of having satellite.The ifc channel shows some really great movies and without it I never would have found this movie.Im not a big fan of the international films because i find that a lot of the don't do a very good job on translating lines.I mean the obvious language barrier leaves you to just believe thats what they are saying but its not that big of a deal i guess.I almost never got to see this AMAZING movie.Good thing i stayed up for it instead of going to bed..well earlier than usual.lol.I hope you all enjoy the hell of this movie and Love this movie just as much as i did.I wish i could type this all in caps but its again the rules i guess thats shouting but it would really show my excitement for the film.I Give It Three Thumbs Way Up!<br /><br />This Movie Blew ME AWAY!\n"
     ]
    }
   ],
   "source": [
    "# In this cell, print out a review your classifier got wrong, along with its label.\n",
    "content, label = nb.find_first_misclassified(0.2)\n",
    "print \"wrong label:\",label\n",
    "print content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are two reasons your system might have misclassified this example? What improvements could you make that may help your system classify this example correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer here.***\n",
    "Maybe because of some negative words and negative sentences which is used in this review that in fact. And also some phrases such as this movie blew me away which is infact a positive sentence but this classifier can not detect it as positive and considers it negative.The problem is that the sequence of words is important in sentiment analysis but in NB the sequence of words is not considered and only occurrence of a word in a class is considered one solution could be using bigram insead of unigram and one way could be detecting phrases in the documents and considering them as a token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.13 (5 pts)**\n",
    "\n",
    "Often times we care about multi-class classification rather than binary classification.\n",
    "\n",
    "How many counts would we need to keep track of if the model were modified to support 5-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer in one or two lines here.***\n",
    "we would need 3more variables for class_total_doc_counts, 3 more variables for class_total_word_counts, 3 * |V| more variables for class_word_counts dictionary, 3 more variable for direction of classes. We also need to update the vocabulary by the unique words in new classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be the new decision rule (i.e., how would the classify function change)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Answer in one or two lines here.***\n",
    "The classifiy function will select the class with maximum probability and returns the label of that class and others would remain same.log priority = count(words in given class)/count(all words in all classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
